{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import hydra\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import imageio\n",
    "import ipdb\n",
    "\n",
    "from compare_traj import supervisedTraj\n",
    "from openai_client_libero import OpenAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML configuration file\n",
    "with open('/home/lgeng/BAKU/baku/cfgs/config_traj.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "device = torch.device(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.96M\n"
     ]
    }
   ],
   "source": [
    "supervised_model_path = \"/home/lgeng/BAKU/baku/exp_local/eval/2024.10.22_supervised_train/deterministic/082008_hidden_dim_256/supervised_model_40.pth\"\n",
    "# \"/home/lgeng/BAKU/baku/exp_local/eval/2024.08.04_supervised_train/deterministic/173530_hidden_dim_256/supervised_model.pth\"\n",
    "# new checkpoint: /home/lgeng/BAKU/baku/exp_local/eval/2024.07.28_supervised_train/deterministic/225134_hidden_dim_256/supervised_model.pth\n",
    "# works well: /home/lgeng/BAKU/baku/exp_local/eval/2024.07.16_supervised_train/deterministic/135045_hidden_dim_256/supervised_model.pth\n",
    "model = supervisedTraj().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supervisedTraj(\n",
       "  (encoder): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (wpe): Embedding(256, 512)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (projection): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(supervised_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_data_path = '/home/lgeng/BAKU/expert_demos/libero/libero_90/KITCHEN_SCENE1_open_the_bottom_drawer_of_the_cabinet.pkl'\n",
    "# pkl_data_path = \"/home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_bottom_drawer_of_the_cabinet_eval_failure_big.pkl\"\n",
    "pkl_data_path = \"/home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it_eval_failure.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it_eval_failure.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {pkl_data_path}\")\n",
    "# read\n",
    "data = pkl.load(open((pkl_data_path), \"rb\"))\n",
    "observations = data[\"observations\"]\n",
    "actions = data[\"actions\"]\n",
    "task_emb = data[\"task_emb\"]\n",
    "aug = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "    )\n",
    "# store\n",
    "episodes = []\n",
    "for i in range(len(observations)):\n",
    "    episode = dict(\n",
    "        observation=observations[i],\n",
    "        action=actions[i],\n",
    "        task_emb=task_emb,\n",
    "    )\n",
    "    episodes.append(episode)\n",
    "    # self._max_episode_len = max(\n",
    "    #     self._max_episode_len,\n",
    "    #     (\n",
    "    #         len(observations[i])\n",
    "    #         if not isinstance(observations[i], dict)\n",
    "    #         else len(observations[i][\"pixels\"])\n",
    "    #     ),\n",
    "    # )\n",
    "    # # if obs_type == 'features':\n",
    "    # self._max_state_dim = max(\n",
    "    #     self._max_state_dim, data[\"states\"][i].shape[-1]\n",
    "    # ) if \"states\" in data.keys() else 0\n",
    "    # self._num_samples += (\n",
    "    #     len(observations[i])\n",
    "    #     if self._obs_type == \"features\"\n",
    "    #     else len(observations[i][\"pixels\"])\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Hydra functions\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.experimental import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if Hydra is already initialized, and clear it if it is\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "# Manually initialize Hydra and get the configuration object\n",
    "initialize(config_path=\"cfgs\")\n",
    "cfg = compose(config_name=\"config_traj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace: /home/lgeng/BAKU/baku\n",
      "Loading /home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it_failure_9500_epoch.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it_eval_failure.pkl\n",
      "Loading /home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it_eval.pkl\n",
      "Loading /home/lgeng/BAKU/expert_demos/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/lgeng/miniforge3/envs/baku/lib/python3.9/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized environment: KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it\n",
      "number of parameters: 6.53M\n",
      "number of parameters: 25.88M\n",
      "number of parameters: 25.88M\n",
      "number of parameters: 25.88M\n",
      "number of parameters: 6.96M\n",
      "loading bc weight: /home/lgeng/BAKU/baku/exp_local/2024.09.29_train/deterministic/034923_hidden_dim_256/snapshot/80000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResnetEncoder(\n",
       "  (resnet18_base): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_1): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (block_2): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (block_3): BasicBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (block_4): BasicBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (lang_proj1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (lang_proj2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (lang_proj3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lang_proj4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (projection_layer): SpatialProjection(\n",
       "    (spatial_softmax): SpatialSoftmax(\n",
       "      (_spatial_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compare_traj import WorkspaceIL as W\n",
    "\n",
    "workspace = W(cfg)\n",
    "\n",
    "# Load weights\n",
    "snapshots = {}\n",
    "# bc\n",
    "bc_snapshot = Path(cfg.bc_weight)\n",
    "if not bc_snapshot.exists():\n",
    "    raise FileNotFoundError(f\"bc weight not found: {bc_snapshot}\")\n",
    "print(f\"loading bc weight: {bc_snapshot}\")\n",
    "snapshots[\"bc\"] = bc_snapshot\n",
    "workspace.load_snapshot(snapshots)\n",
    "\n",
    "language_projector = workspace.agent.language_projector\n",
    "language_projector.eval()\n",
    "encoder = workspace.agent.encoder\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "\n",
    "VIDEO_FILENAME = \"videos\"\n",
    "filename_with_extension = os.path.basename(pkl_data_path)\n",
    "filename_without_extension, _ = os.path.splitext(filename_with_extension)\n",
    "save_dir = os.path.join(os.path.dirname(pkl_data_path), VIDEO_FILENAME, filename_without_extension)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "height, width = 128, 128\n",
    "\n",
    "for i, episode in enumerate(episodes):\n",
    "    traj = episode[\"observation\"]['pixels']\n",
    "    # Save the video in the created directory\n",
    "    images = [img.astype('uint8') for img in traj]\n",
    "    video_path = os.path.join(save_dir, f'{i}.mp4')\n",
    "    imageio.mimsave(video_path, images, fps=20)\n",
    "    \n",
    "    traj_np = traj\n",
    "    traj = torch.stack(\n",
    "        [aug(traj[i]) for i in range(len(traj))]\n",
    "    ).to(device).float()\n",
    "\n",
    "    task_emb = torch.tensor(task_emb).to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lang_features = language_projector(task_emb[None]).repeat(traj.shape[0], 1)\n",
    "        traj = encoder(traj, lang=lang_features)\n",
    "    \n",
    "    traj_batch = traj.unsqueeze(0)\n",
    "    label_batch = torch.tensor([0]).to(device)\n",
    "    batch = [traj_batch, label_batch]\n",
    "\n",
    "    # Predict\n",
    "    pred = model.show_prob(batch)\n",
    "\n",
    "    pred_np = pred[0].detach().cpu().numpy()[0]\n",
    "\n",
    "    # Initialize VideoWriter for each episode\n",
    "    output_video_path = os.path.join(save_dir, f'output_{i}.mp4')\n",
    "\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    for i, frame in enumerate(traj_np):\n",
    "        # Capture frames in the video \n",
    "        ret, frame = cap.read() \n",
    "        prob_text = f\"Prob: {pred_np[i]:.2f}\" if i < len(pred_np) else \"\"\n",
    "        # Use putText() method for inserting text on video \n",
    "        cv2.putText(frame,  \n",
    "                    prob_text,  \n",
    "                    (25, 25),  \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,  \n",
    "                    (255, 255, 255), \n",
    "                    2,  \n",
    "                    cv2.LINE_4) \n",
    "        frames.append(frame)\n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "    # Save the video\n",
    "    imageio.mimsave(output_video_path, frames, fps=20)\n",
    "\n",
    "    # Get the index of first prob in the pred sequence that is below .5\n",
    "    below_half = np.where(pred_np < .55)[0]\n",
    "    indices.append(below_half[0] if len(below_half) > 0 else None)\n",
    "\n",
    "# close all windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_traj_gpt(task, data_path, img_save_dir, traj_idx, video_save_dir):\n",
    "    client = OpenAIClient(task, data_path, img_save_dir)\n",
    "    every_n = client.every_n\n",
    "    likelihood_responses = client.get_likelihood_responses(traj_idx)\n",
    "\n",
    "    # Convert likelihood responses to numpy array\n",
    "    likelihood_responses_binary = [1 if \"Yes\" in response else 0 for response in likelihood_responses]\n",
    "    likelihood_responses_binary = np.array(likelihood_responses_binary)\n",
    "\n",
    "    # Make dir\n",
    "    os.makedirs(video_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the episode\n",
    "    episode = episodes[traj_idx]\n",
    "    traj = episode[\"observation\"]['pixels']\n",
    "    # Save the video in the created directory\n",
    "    images = [img.astype('uint8') for img in traj]\n",
    "    video_path = os.path.join(video_save_dir, f'{traj_idx}.mp4')\n",
    "    imageio.mimsave(video_path, images, fps=20)\n",
    "    \n",
    "    # Set the output video path\n",
    "    output_video_path = os.path.join(video_save_dir, f'output_gpt_{traj_idx}.mp4')\n",
    "\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frames = []\n",
    "    for i, frame in enumerate(traj):\n",
    "        # Convert index to every nth frame index\n",
    "        new_idx = i // every_n\n",
    "        # Capture frames in the video \n",
    "        ret, frame = cap.read() \n",
    "        prob_text = f\"Prob: {likelihood_responses_binary[new_idx]:.2f}\" if new_idx < len(likelihood_responses_binary) else \"\"\n",
    "        # Use putText() method for inserting text on video \n",
    "        cv2.putText(frame,  \n",
    "                    prob_text,  \n",
    "                    (25, 25),  \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,  \n",
    "                    (255, 255, 255), \n",
    "                    2,  \n",
    "                    cv2.LINE_4) \n",
    "        frames.append(frame)\n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "    # Save the video\n",
    "    imageio.mimsave(output_video_path, frames, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_traj(data_path, traj_idx, video_save_dir, thres=0.5):\n",
    "    data = pkl.load(open((data_path), \"rb\"))\n",
    "    observations = data[\"observations\"]\n",
    "    actions = data[\"actions\"]\n",
    "    task_emb = data[\"task_emb\"]\n",
    "    aug = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        )\n",
    "    episode = dict(\n",
    "        observation=observations[traj_idx],\n",
    "        action=actions[traj_idx],\n",
    "        task_emb=task_emb,\n",
    "    )\n",
    "    traj = episode[\"observation\"]['pixels']\n",
    "    # Save the video in the created directory\n",
    "    images = [img.astype('uint8') for img in traj]\n",
    "    video_path = os.path.join(video_save_dir, f'{traj_idx}.mp4')\n",
    "    imageio.mimsave(video_path, images, fps=20)\n",
    "    \n",
    "    traj_np = traj\n",
    "    traj = torch.stack(\n",
    "        [aug(traj[i]) for i in range(len(traj))]\n",
    "    ).to(device).float()\n",
    "\n",
    "    task_emb = torch.tensor(task_emb).to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lang_features = language_projector(task_emb[None]).repeat(traj.shape[0], 1)\n",
    "        traj = encoder(traj, lang=lang_features)\n",
    "    \n",
    "    traj_batch = traj.unsqueeze(0)\n",
    "    label_batch = torch.tensor([0]).to(device)\n",
    "    batch = [traj_batch, label_batch]\n",
    "\n",
    "    # Predict\n",
    "    pred = model.show_prob(batch)\n",
    "\n",
    "    pred_np = pred[0].detach().cpu().numpy()[0]\n",
    "\n",
    "    # # Thresholding\n",
    "    # pred_np = np.where(pred_np < thres, 0, 1)\n",
    "\n",
    "    # Initialize VideoWriter for each episode\n",
    "    output_video_path = os.path.join(video_save_dir, f'output_{traj_idx}.mp4')\n",
    "\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    for i, frame in enumerate(traj_np):\n",
    "        # Capture frames in the video \n",
    "        ret, frame = cap.read() \n",
    "        prob_text = f\"Prob: {pred_np[i]:.2f}\" if i < len(pred_np) else \"\"\n",
    "        # Use putText() method for inserting text on video \n",
    "        cv2.putText(frame,  \n",
    "                    prob_text,  \n",
    "                    (25, 25),  \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,  \n",
    "                    (255, 255, 255), \n",
    "                    2,  \n",
    "                    cv2.LINE_4) \n",
    "        frames.append(frame)\n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "    # Save the video\n",
    "    imageio.mimsave(output_video_path, frames, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = \"open_the_bottom_drawer_of_the_cabinet\"\n",
    "# img_save_dir = '/home/lgeng/BAKU/baku/libero/libero_90/KITCHEN_SCENE1_open_the_bottom_drawer_of_the_cabinet'\n",
    "# video_save_dir = '/home/lgeng/BAKU/baku/libero/libero_90/KITCHEN_SCENE1_open_the_bottom_drawer_of_the_cabinet/videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"open_the_bottom_drawer_of_the_cabinet\"\n",
    "img_save_dir = '/home/lgeng/BAKU/baku/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_bottom_drawer_of_the_cabinet_eval_failure_big'\n",
    "video_save_dir = '/home/lgeng/BAKU/baku/libero/libero_single_task_success_failure/KITCHEN_SCENE1_open_the_bottom_drawer_of_the_cabinet_eval_failure_big/videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_idx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to determine the likelihood based on this image alone.\n",
      "I'm unable to determine the likelihood based on these images.\n",
      "No\n",
      "No.\n",
      "No\n",
      "No.\n",
      "No\n",
      "No.\n",
      "No\n",
      "No.\n",
      "No\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "No\n",
      "No.\n",
      "No.\n",
      "No.\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "eval_traj_gpt(task, pkl_data_path, img_save_dir, traj_idx, video_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_traj(pkl_data_path, traj_idx, video_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
